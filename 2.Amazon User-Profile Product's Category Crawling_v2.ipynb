{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon User-Profile Product's Category Crawling\n",
    "\n",
    "Step2 Crawling products categories\n",
    "\n",
    "Crawl the products categories from Step1 Purchasing-history-record for grouping or knowing relationship of products and easily analyzing.\n",
    "\n",
    "### Before running the crawling code you should:\n",
    "- Make sure your data from Step 1 are completely and non duplicated.\n",
    "- `Using API` on [`Scraper API website']('https://www.scraperapi.com/?fp_ref=viktoriia91)\n",
    "- Put cookie.txt copy from AMZ to be the request cookies.\n",
    "- Make sure internet is stable and well connected.\n",
    "### When running the crawling code you should:\n",
    "- Check the `crawled data amount` to make sure resonable.\n",
    "- Follow the instruction of the note.\n",
    "### After running the crawling code you should:\n",
    "- Check the data amounts and `Correct file name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "user_agent = UserAgent()\n",
    "ua = user_agent\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "from urllib.parse import urlencode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Model Name\n",
    "modelname = 'Dell UltraSharp U3415W 34-Inch Curved LED-Lit Monitor'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Step1 Data from single model\n",
    "crawl_data = pd.read_csv(f'Step1/AMZ_PH_STEP1_{modelname}.csv', encoding='utf-8')\n",
    "print('Data amounts:', len(crawl_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cookie.txt', 'r') as f:\n",
    "    output = f.read()\n",
    "cookies = {}\n",
    "for line in output.split(';'):\n",
    "    name, value = line.strip().split('=', 1)\n",
    "    cookies[name] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = {\n",
    "    \"http\": \"http://scraperapi:f6f47b89744ec60336e92cb702d9c31e@proxy-server.scraperapi.com:8001\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request the products page that on purpose\n",
    "def request_web(url, idx):\n",
    "    headers = {\"User-Agent\": ua.random}\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url, headers=headers, proxies=proxies, verify=False, cookies=cookies\n",
    "        )\n",
    "        txt = response.text\n",
    "        return txt\n",
    "    except:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get category and easy cleaning\n",
    "def get_content(soap):\n",
    "    category = soap.find_all(\"a\", class_=\"a-link-normal a-color-tertiary\")\n",
    "    price = soap.find(\"span\", class_='a-offscreen')\n",
    "    try:\n",
    "        brand = soap.find(text='Brand').findNext('td').text\n",
    "    except:\n",
    "        brand = 'None'\n",
    "    if category != []:\n",
    "        category = [\n",
    "            str(t)\n",
    "            .split('\">')[1]\n",
    "            .split('</a>')[0]\n",
    "            .replace(\" \", \"\")\n",
    "            .replace('\\n', \"\")\n",
    "            .replace('&amp;', \"\")\n",
    "            for t in category\n",
    "        ]\n",
    "        try:\n",
    "            price = str(price).split('$')[1].replace('</span>', '')\n",
    "        except:\n",
    "            price = 'Non-Available'\n",
    "        content_dict = {'category': category, 'price': price, 'Brand': brand}\n",
    "        return content_dict\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All info including category into dataframe\n",
    "def scrape(arg):\n",
    "    url, idx = arg\n",
    "    tmp_format = pd.DataFrame(\n",
    "        {\n",
    "            'Asin': [],\n",
    "            'Brand': [],\n",
    "            'Category': [],\n",
    "            'Sub_category': [],\n",
    "            'Breakdown_category': [],\n",
    "            'Price': [],\n",
    "        }\n",
    "    )\n",
    "    if request_web(url, idx) != False:\n",
    "        soap = BeautifulSoup(request_web(url, idx), \"lxml\")\n",
    "        if get_content(soap) != False:\n",
    "            brand = get_content(soap)['Brand']\n",
    "            category = get_content(soap)['category']\n",
    "            price = get_content(soap)['price']\n",
    "            tmp_format = tmp_format.append(\n",
    "                {\n",
    "                    'Asin': url.split('/dp/')[1],\n",
    "                    'Category': [category[1]],\n",
    "                    'Sub_category': [category[-2]],\n",
    "                    'Breakdown_category': category[-1],\n",
    "                    'Price': [price],\n",
    "                    'Brand': [brand],\n",
    "                },\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            time.sleep(2)\n",
    "\n",
    "    return tmp_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to store the crawled data\n",
    "final_format = pd.DataFrame(\n",
    "    {\n",
    "        'Asin': [],\n",
    "        'Brand': [],\n",
    "        'Category': [],\n",
    "        'Sub_category': [],\n",
    "        'Breakdown_category': [],\n",
    "        'Price': [],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Execution Code\n",
    "def run(asin_list):\n",
    "    base_url = 'https://www.amazon.com'\n",
    "    url_list = [f\"{base_url}/dp/{asin}\" for asin in asin_list]\n",
    "    idx_list = []\n",
    "    for idx in range(len(crawl_data) - 1):\n",
    "        idx_list.append(idx)\n",
    "    arg = list(zip(url_list, idx_list))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:\n",
    "        for result in tqdm.tqdm_notebook(\n",
    "            (executor.map(scrape, arg)), total=len(url_list)\n",
    "        ):\n",
    "            global final_format\n",
    "            final_format = final_format.append(result, ignore_index=True)\n",
    "        executor.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No matter it failed or not just keep running next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executive every products in step one on single model\n",
    "run(crawl_data['Asin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out the crawled data and prepare the failing data for another try\n",
    "notSuccessInFinalFormat_Asin = pd.concat(\n",
    "    [crawl_data['Asin'], final_format['Asin'], final_format['Asin']]\n",
    ").drop_duplicates(keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Success:' + str(len(final_format)),\n",
    "    'Falied:' + str(len(notSuccessInFinalFormat_Asin)),\n",
    "    'Total Data in Step1:' + str(len(crawl_data)),\n",
    "    final_format.head(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the crawled data\n",
    "final_format.to_csv(f'Step2/AMZ_PH_STEP2_{modelname}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep running Cells below from here until crawl enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(notSuccessInFinalFormat_Asin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notSuccessInFinalFormat_Asin = pd.concat(\n",
    "    [crawl_data['Asin'], final_format['Asin'], final_format['Asin']]\n",
    ").drop_duplicates(keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Success:' + str(len(final_format)),\n",
    "    'Failed:' + str(len(notSuccessInFinalFormat_Asin)),\n",
    "    'Total Data in Step1:' + str(len(crawl_data)),\n",
    "    final_format.head(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_format.to_csv(f'Step2/AMZ_PH_STEP2_{modelname}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
